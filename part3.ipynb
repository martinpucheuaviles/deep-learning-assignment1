{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,exp,fsum\n",
    "from data import load_mnist,load_synth\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "        self.x = None #first layer input nodes\n",
    "        self.W = None #first layer weights matrix\n",
    "        self.b = None #input bias\n",
    "        self.k = None #first layer linear output\n",
    "        self.h = None #sigmoid activation output\n",
    "        self.V = None #second layer weight matrix\n",
    "        self.c = None #second layer bias\n",
    "        self.o = None #softmax input\n",
    "        self.y = None #softmax output\n",
    "        self.t = None #target vector        \n",
    "\n",
    "        # (self.xtrain, self.ytrain), (self.xval, self.yval), self.num_cls = load_mnist()\n",
    "        (self.xtrain, self.ytrain), (self.xval, self.yval), self.num_cls = load_synth()\n",
    "\n",
    "\n",
    "    def default_init(self):\n",
    "        self.set_nodes()\n",
    "        self.set_weights_W()\n",
    "        self.set_weights_V()\n",
    "    \n",
    "    def set_nodes(self,x=784,k=300,h=300,o=10,y=10):\n",
    "        \"\"\"\n",
    "        Inititalize list of nodes with specific sizes\n",
    "        Also works for reset values for a new fordward pass\n",
    "        Parameters:\n",
    "            x: size of first layer input nodes\n",
    "            k: size of first layer linear output\n",
    "            h: size of sigmoid nodes layer\n",
    "            o: size of softmax input layer\n",
    "            y: size of softmax output layer\n",
    "        \"\"\"\n",
    "        self.x = [0. for _ in range(x)]\n",
    "        self.k = [0. for _ in range(k)]\n",
    "        self.h = [0. for _ in range(h)]\n",
    "        self.o = [0. for _ in range(o)]\n",
    "        self.y = [0. for _ in range(y)]\n",
    "\n",
    "        #bias\n",
    "        self.b = [0. for _ in range(k)]\n",
    "        self.c = [0. for _ in range(o)]    \n",
    "\n",
    "\n",
    "    def set_weights_W(self,mu=0.0,sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize weights matrix W\n",
    "        Parameters:\n",
    "            -mu     : mean of the normal distribution from where the random weights are generated\n",
    "            -sigma  : standar deviation of the normal distribution from where the random weights are generated\n",
    "        \"\"\"\n",
    "        self.W = [[np.random.normal(loc=mu,scale=sigma) for _ in range(len(self.k))] for __ in range(len(self.x))]\n",
    "        \n",
    "\n",
    "    def set_weights_V(self,mu=0.0,sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize weights matrix V\n",
    "        Parameters:\n",
    "            -mu     : mean of the normal distribution from where the random weights are generated\n",
    "            -sigma  : standar deviation of the normal distribution from where the random weights are generated\n",
    "        \"\"\"\n",
    "        self.V = [[np.random.normal(loc=mu,scale=sigma) for _ in range(len(self.o))] for __ in range(len(self.h))]\n",
    "\n",
    "    def set_derivative_lists(self):\n",
    "        dl_dy = [0.  for _ in range(len(self.y))]                               #derivatives of the loss wrt softmax output\n",
    "        dy_do = [[0. for _ in range(len(self.y))] for __ in range(len(self.o))] #derivatives of the softmax output wrt softmax input\n",
    "        dl_do = [0.  for _ in range(len(self.o))]                               #derivatives of the loss wrt softmax input\n",
    "        do_dh = [[0. for _ in range(len(self.k))] for __ in range(len(self.o))] #derivatives of the softmax input wrt to sigmoid output\n",
    "        dl_dh = [0.  for _ in range(len(self.h))]                               #derivatives of the loss wrt sigmoid output\n",
    "        dl_dv = [[0. for _ in range(len(self.o))] for __ in range(len(self.h))] #derivatives of the loss wrt to weights V\n",
    "        dl_dc = [0.  for _ in range(len(self.o))]                               #derivatives of the loss wrt to bias C\n",
    "        dl_dk = [0.  for _ in range(len(self.k))]                               #derivatives of the loss wrt to sigmoid input\n",
    "        dh_dk = [0.  for _ in range(len(self.k))]                               #derivatives of the sigmoid output wrt to sigmoid input (only interested in same i-index e.g dHi/dK))\n",
    "        dl_dw = [[0. for _ in range(len(self.k))] for __ in range(len(self.x)) ]#derivatives of the loss wrt to weights W\n",
    "        dl_db = [0.  for _ in range(len(self.k))]                               #derivatives of the loss wrt to bias B\n",
    "\n",
    "        return dl_dy,dy_do,dl_do,do_dh,dl_dh,dl_dv,dl_dc,dl_dk,dh_dk,dl_dw,dl_db\n",
    "\n",
    "    def cross_entropy(self, y, true_index):\n",
    "        # ytrain goes from 0 to 9\n",
    "        return -log(y[true_index])\n",
    "\n",
    "    def train(self,x,true_y,alpha,verbose=False):\n",
    "        # TODO START WITH FORDWARD PASS MATRIX MODE\n",
    "        \"\"\" ######################################## FORWARD #####################################\"\"\"\n",
    "        \n",
    "        self.set_nodes(x=2,k=3,h=3,o=2,y=2)\n",
    "\n",
    "        self.x = x\n",
    "        self.t = true_y\n",
    "\n",
    "        self.k = self.W.dot(self.x) + self.b\n",
    "\n",
    "        if verbose: self.report_f(self.t)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_epoch(self,alpha=0.02,rounds=None,verbose=False):\n",
    "        self.losses = []\n",
    "        counter = 0\n",
    "\n",
    "        for x,true_y in zip(self.xtrain,self.ytrain):\n",
    "            # target_i = [0,1] if self.ytrain[i] == 0 else [1,0]\n",
    "            \n",
    "            loss = self.train(x,true_y,alpha=alpha,verbose=verbose)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # self.report_f()\n",
    "            if rounds != None:\n",
    "                if counter < rounds:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    break        \n",
    "\n",
    "    def report_f(self,target=[0,0],loss=0):\n",
    "        print(f\"##### FORWARD #######\")\n",
    "        print(f\"-t = {target}\\tloss = {loss}\")\n",
    "        print(f\"-y = {self.y}\")\n",
    "        print(f\"-o = {self.o}\")\n",
    "        print(f\"-V = {self.V}\")\n",
    "        print(f\"-c = {self.c}\")\n",
    "        print(f\"-h = {self.h}\")\n",
    "        print(f\"-k = {self.k}\")\n",
    "        print(f\"-W = {self.W}\")\n",
    "        print(f\"-b = {self.b}\")\n",
    "        print(f\"-x = {self.x}\")                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4036/1679186571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# dnn.default_init()*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4036/2822366960.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, alpha, rounds, verbose)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# target_i = [0,1] if self.ytrain[i] == 0 else [1,0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4036/2822366960.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, true_y, alpha, verbose)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "dnn = DNN()\n",
    "\n",
    "dnn.set_nodes(x=2,k=3,h=3,o=2,y=2)\n",
    "dnn.set_weights_W()\n",
    "dnn.set_weights_V()\n",
    "# dnn.default_init()*\n",
    "\n",
    "dnn.train_epoch(rounds=2,verbose=True)\n",
    "\n",
    "\n",
    "# dl_dy,dy_do,dl_do,do_dh,dl_dh,dl_dv,dl_dc,dl_dk,dh_dk,dl_dw,dl_db = dnn.set_derivative_lists()\n",
    "\n",
    "# print(do_dh)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.06\n"
     ]
    }
   ],
   "source": [
    "true_y = dnn.ytrain[0]\n",
    "x = dnn.xtrain[0]\n",
    "print(true_y)\n",
    "# print(dnn.xtrain[[0,0,0,1] -1])\n",
    "\n",
    "fake_o = [0.3, 0.1, 0.05, 0.05, 0.1111, 0.06,0.04, 0.06, 0.1, 0.2]\n",
    "\n",
    "\n",
    "print(fake_o[-4 -1])\n",
    "# print(fake_o[-1 -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(dnn.ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8349477286608628"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(loc=0,scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances:     60000\n",
      "number of input nodes:            2\n",
      "(given) number of output nodes:   10\n",
      "(given) hidden layer size:        300\n"
     ]
    }
   ],
   "source": [
    "# type(q5_dnn.xtrain)\n",
    "# print(np.unique(q5_dnn.xtrain))\n",
    "print(f\"number of training instances:     {len(dnn.xtrain)}\")\n",
    "print(f\"number of input nodes:            {len(dnn.xtrain[0])}\")\n",
    "print(f\"(given) number of output nodes:   10\")\n",
    "print(f\"(given) hidden layer size:        300\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1d88f1d6af7274392319340ad589157e5034eb25853bd7ff5b502ff0dd39369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
