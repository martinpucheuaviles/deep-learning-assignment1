{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,exp,fsum\n",
    "from data import load_mnist,load_synth\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(o):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -o : vector of softmax input\n",
    "      -i : index i of numerator \n",
    "  \"\"\"\n",
    "  return np.exp(o) / fsum(np.exp(o))  \n",
    "\n",
    "def cross_entropy_loss(y, true_y):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        -y: vector output of softmax\n",
    "        -true_y: true target value (or index)\n",
    "    \"\"\"\n",
    "    # ytrain goes from 0 to 9\n",
    "    return -log(y[true_y])  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "        self.x = None #first layer input nodes\n",
    "        self.W = None #first layer weights matrix\n",
    "        self.b = None #input bias\n",
    "        self.k = None #first layer linear output\n",
    "        self.h = None #sigmoid activation output\n",
    "        self.V = None #second layer weight matrix\n",
    "        self.c = None #second layer bias\n",
    "        self.o = None #softmax input\n",
    "        self.y = None #softmax output\n",
    "        self.t = None #target vector        \n",
    "\n",
    "        # (self.xtrain, self.ytrain), (self.xval, self.yval), self.num_cls = load_mnist()\n",
    "        (self.xtrain, self.ytrain), (self.xval, self.yval), self.num_cls = load_synth()\n",
    "\n",
    "\n",
    "    def default_init(self):\n",
    "        self.set_nodes()\n",
    "        self.set_weights_W()\n",
    "        self.set_weights_V()\n",
    "    \n",
    "    def set_nodes(self,x=784,k=300,h=300,o=10,y=10):\n",
    "        \"\"\"\n",
    "        Inititalize list of nodes with specific sizes\n",
    "        Also works for reset values for a new fordward pass\n",
    "        Parameters:\n",
    "            x: size of first layer input nodes\n",
    "            k: size of first layer linear output\n",
    "            h: size of sigmoid nodes layer\n",
    "            o: size of softmax input layer\n",
    "            y: size of softmax output layer\n",
    "        \"\"\"\n",
    "        self.x = np.zeros((x,1))\n",
    "        self.k = np.zeros((k,1))\n",
    "        self.h = np.zeros((h,1))\n",
    "        self.o = np.zeros((o,1))\n",
    "        self.y = np.zeros((y,1))\n",
    "\n",
    "        #bias\n",
    "        self.b = np.zeros((k,1))\n",
    "        self.c = np.zeros((o,1))\n",
    "\n",
    "\n",
    "    def set_weights_W(self,mu=0.0,sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize weights matrix W\n",
    "        Parameters:\n",
    "            -mu     : mean of the normal distribution from where the random weights are generated\n",
    "            -sigma  : standar deviation of the normal distribution from where the random weights are generated\n",
    "        \"\"\"\n",
    "        x_rows = np.shape(self.x)[0]\n",
    "        k_rows = np.shape(self.k)[0]\n",
    "        # self.W = np.random.normal(loc=mu,scale=sigma,size=(k_rows,x_rows))\n",
    "        self.W = np.array([[1., -1.], [1.,-1.], [1., -1.]])\n",
    "        \n",
    "        \n",
    "\n",
    "    def set_weights_V(self,mu=0.0,sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize weights matrix V\n",
    "        Parameters:\n",
    "            -mu     : mean of the normal distribution from where the random weights are generated\n",
    "            -sigma  : standar deviation of the normal distribution from where the random weights are generated\n",
    "        \"\"\"\n",
    "        h_cols = np.shape(self.h)[0]\n",
    "        o_cols = np.shape(self.o)[0]\n",
    "        # self.V = np.random.normal(loc=mu,scale=sigma,size=(o_cols,h_cols))\n",
    "        self.V = np.array([[1., -1., -1.], [1.,-1., -1.]])\n",
    "\n",
    "    def set_derivative_lists(self):\n",
    "        # dl_dy = [0.  for _ in range(len(self.y))]                               #derivatives of the loss wrt softmax output\n",
    "        # dy_do = [[0. for _ in range(len(self.y))] for __ in range(len(self.o))] #derivatives of the softmax output wrt softmax input\n",
    "        # dl_do = [0.  for _ in range(len(self.o))]                               #derivatives of the loss wrt softmax input\n",
    "        # do_dh = [[0. for _ in range(len(self.k))] for __ in range(len(self.o))] #derivatives of the softmax input wrt to sigmoid output\n",
    "        # dl_dh = [0.  for _ in range(len(self.h))]                               #derivatives of the loss wrt sigmoid output\n",
    "        # dl_dv = [[0. for _ in range(len(self.o))] for __ in range(len(self.h))] #derivatives of the loss wrt to weights V\n",
    "        # dl_dc = [0.  for _ in range(len(self.o))]                               #derivatives of the loss wrt to bias C\n",
    "        # dl_dk = [0.  for _ in range(len(self.k))]                               #derivatives of the loss wrt to sigmoid input\n",
    "        # dh_dk = [0.  for _ in range(len(self.k))]                               #derivatives of the sigmoid output wrt to sigmoid input (only interested in same i-index e.g dHi/dK))\n",
    "        # dl_dw = [[0. for _ in range(len(self.k))] for __ in range(len(self.x)) ]#derivatives of the loss wrt to weights W\n",
    "        # dl_db = [0.  for _ in range(len(self.k))]                               #derivatives of the loss wrt to bias B\n",
    "\n",
    "        y_rows = np.shape(self.y)[0]\n",
    "        o_rows = np.shape(self.o)[0]\n",
    "        h_rows = np.shape(self.h)[0]\n",
    "        k_rows = np.shape(self.k)[0]\n",
    "        x_rows = np.shape(self.x)[0]\n",
    "\n",
    "        self.dl_dy = np.zeros((y_rows,1))                                #derivatives of the loss wrt softmax output\n",
    "        self.dy_do = np.zeros((y_rows,o_rows))                           #derivatives of the softmax output wrt softmax input\n",
    "        self.dl_do = np.zeros((1,o_rows))                                #derivatives of the loss wrt softmax input\n",
    "        self.do_dh = np.zeros((o_rows,h_rows))                           #derivatives of the softmax input wrt to sigmoid output\n",
    "        self.dl_dh = np.zeros((1,h_rows))                                #derivatives of the loss wrt sigmoid output\n",
    "        self.do_dv = np.zeros((o_rows,))\n",
    "        self.dl_dv = np.zeros((o_rows,h_rows))                           #derivatives of the loss wrt to weights V\n",
    "        self.dl_dc = np.zeros((1,o_rows))                                #derivatives of the loss wrt to bias C\n",
    "        self.dl_dk = np.zeros((1,k_rows))                                #derivatives of the loss wrt to sigmoid input\n",
    "        self.dh_dk = np.zeros((1,k_rows))                                #derivatives of the sigmoid output wrt to sigmoid input (only interested in same i-index e.g dHi/dK))\n",
    "        self.dl_dw = np.zeros((k_rows,x_rows))                           #derivatives of the loss wrt to weights W\n",
    "        self.dl_db = np.zeros((1,k_rows))                                #derivatives of the loss wrt to bias B        \n",
    "\n",
    "    def onehot_encode_true_y(self):\n",
    "        \"\"\"\n",
    "        Creates a matrix of size (n,n) fulls of zeros except from the last column and i-th row, where i is the index of the target\n",
    "        \"\"\"\n",
    "        y_rows = np.shape(self.y)[0]\n",
    "\n",
    "        encoded = np.zeros((y_rows,y_rows),dtype=int)\n",
    "        encoded[self.t,y_rows-1] = 1\n",
    "\n",
    "        return  encoded\n",
    "\n",
    "    def gradient_dy_do(self):\n",
    "        y_ = self.y.reshape(-1,1)\n",
    "\n",
    "        return np.diagflat(y_) - np.dot(y_, y_.T)       \n",
    "\n",
    "    def forward_pass(self,x,true_y,verbose=False):\n",
    "        self.set_nodes(x=2,k=3,h=3,o=2,y=2)\n",
    "        self.x = x.reshape(2,1)\n",
    "        self.t = true_y\n",
    "\n",
    "        self.k = self.W.dot(self.x) + self.b\n",
    "        \n",
    "        self.h = sigmoid(self.k)\n",
    "        self.o = self.V.dot(self.h) + self.c\n",
    "        self.y = softmax(self.o)\n",
    "\n",
    "        loss = cross_entropy_loss(self.y,self.t)\n",
    "\n",
    "        if verbose: self.report_f(target=self.t,loss=loss)\n",
    "        \n",
    "        return loss        \n",
    "    \n",
    "    def backward_pass(self,alpha=0.01,verbose=False):\n",
    "        self.set_derivative_lists()\n",
    "        \n",
    "        true_y_encoded = self.onehot_encode_true_y() # TODO: OJO CON ESTO AL PASAR A RED GRANDE\n",
    "        # print(f\"encoded: {true_y_encoded.tolist()} \\t shape: {true_y_encoded.shape}\")\n",
    "\n",
    "        \"\"\" ############# DL / DY #############\"\"\"\n",
    "        # dl_dy = np.where(dl_dy.__index__ , dl_dy, 10*a)\n",
    "        self.dl_dy = true_y_encoded.dot((1/self.y))\n",
    "\n",
    "        \"\"\" ############# DY / DO #############\"\"\"\n",
    "\n",
    "        self.dy_do = self.gradient_dy_do()\n",
    "        # print(dy_do)\n",
    "\n",
    "        \"\"\" ############# DL / DO #############\"\"\"\n",
    "        print(f\"\\n {self.dl_dy.shape}*{self.dy_do.shape} = {(self.dl_dy * self.dy_do).shape}\")\n",
    "        self.dl_do = (self.dl_dy * self.dy_do).sum(axis=0) # TODO: OJO AQUI \n",
    "        \n",
    "        \"\"\" ############# DL / DV #############\"\"\"\n",
    "        \"\"\"|->        and DO / DV      \"\"\"\n",
    "        self.do_dv = self.h\n",
    "        self.dl_dv = self.dl_do * self.do_dv # dl_do.shape=(2,) and h.shape=(3,1) --> (dl_do * h).shape = (3,2)\n",
    "        \n",
    "        \"\"\" ############# DL / DC #############\"\"\"\n",
    "        self.dl_dc = self.dl_do\n",
    "        \n",
    "        \"\"\" ############# DO / DH #############\"\"\"\n",
    "        \n",
    "        self.do_dh = self.V\n",
    "\n",
    "        \"\"\" ############# DL / DH #############\"\"\"\n",
    "        # self.dl_dh = (self.do_dh * self.dl_do).sum(axis=0)\n",
    "        # dl_do =(self.dl_dy * self.dy_do).sum(axis=0)\n",
    "        # print(dl_do)\n",
    "\n",
    "        if verbose: self.report_derivs()\n",
    "\n",
    "\n",
    "    def train_epoch(self,alpha=0.02,rounds=None,verbose=False):\n",
    "        self.losses = []\n",
    "        counter = 0\n",
    "\n",
    "        for x,true_y in zip(self.xtrain,self.ytrain):\n",
    "            # target_i = [0,1] if self.ytrain[i] == 0 else [1,0]\n",
    "            \n",
    "            x=np.array([1,-1])\n",
    "\n",
    "            loss = self.forward_pass(x,true_y,verbose=verbose)\n",
    "            self.backward_pass(alpha=alpha,verbose=verbose)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # self.report_f()\n",
    "            if rounds != None:\n",
    "                if counter < rounds:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    break        \n",
    "\n",
    "    def report_f(self,target=[0,0],loss=0):\n",
    "        print(f\"##### FORWARD #######\")\n",
    "        print(f\"-t = {target}\\tloss = {loss}\")\n",
    "        print(f\"-y = {self.y.tolist()}\")\n",
    "        print(f\"-o = {self.o.tolist()}\")\n",
    "        print(f\"-V = {self.V.tolist()}\\tshape={self.V.shape}\")\n",
    "        print(f\"-c = {self.c.tolist()}\")\n",
    "        print(f\"-h = {self.h.tolist()}\")\n",
    "        print(f\"-k = {self.k.tolist()}\")\n",
    "        print(f\"-W = {self.W.tolist()}\")\n",
    "        # print(f\"-W = {self.W}\")\n",
    "        print(f\"-b = {self.b.tolist()}\")\n",
    "        print(f\"-x = {self.x.tolist()}\")                \n",
    "\n",
    "    def report_derivs(self):\n",
    "        print(f\"\\t|##### DERIVS #######\")\n",
    "        print(f\"\\t|dl_dy = {self.dl_dy.tolist()}\")\n",
    "        print(f\"\\t|dy_do = {self.dy_do.tolist()}\")\n",
    "        print(f\"\\t|dl_do = {self.dl_do.tolist()}\")\n",
    "        print(f\"\\t|dl_dv = {self.dl_dv.tolist()}\")\n",
    "        print(f\"\\t|dl_dc = {self.dl_dc.tolist()}\")\n",
    "        print(f\"\\t|do_dh = {self.do_dh.tolist()}\")\n",
    "        print(f\"\\t|dl_dh = {self.dl_dh.tolist()}\")\n",
    "        print(f\"\\t|dh_dk = {self.dh_dk.tolist()}\")\n",
    "        print(f\"\\t|dl_dk = {self.dl_dk.tolist()}\")\n",
    "        print(f\"\\t|dl_dw = {self.dl_dw.tolist()}\")\n",
    "        print(f\"\\t|dl_db = {self.dl_db.tolist()}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### FORWARD #######\n",
      "-t = 1\tloss = 0.6931471805599453\n",
      "-y = [[0.5], [0.5]]\n",
      "-o = [[-0.8807970779778823], [-0.8807970779778823]]\n",
      "-V = [[1.0, -1.0, -1.0], [1.0, -1.0, -1.0]]\tshape=(2, 3)\n",
      "-c = [[0.0], [0.0]]\n",
      "-h = [[0.8807970779778823], [0.8807970779778823], [0.8807970779778823]]\n",
      "-k = [[2.0], [2.0], [2.0]]\n",
      "-W = [[1.0, -1.0], [1.0, -1.0], [1.0, -1.0]]\n",
      "-b = [[0.0], [0.0], [0.0]]\n",
      "-x = [[1], [-1]]\n",
      "\n",
      " (2, 1)*(2, 2) = (2, 2)\n",
      "\t|##### DERIVS #######\n",
      "\t|dl_dy = [[0.0], [2.0]]\n",
      "\t|dy_do = [[0.25, -0.25], [-0.25, 0.25]]\n",
      "\t|dl_do = [-0.5, 0.5]\n",
      "\t|dl_dv = [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "\t|dl_dc = [-0.5, 0.5]\n",
      "\t|do_dh = [[1.0, -1.0, -1.0], [1.0, -1.0, -1.0]]\n",
      "\t|dl_dh = [[0.0, 0.0, 0.0]]\n",
      "\t|dh_dk = [[0.0, 0.0, 0.0]]\n",
      "\t|dl_dk = [[0.0, 0.0, 0.0]]\n",
      "\t|dl_dw = [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n",
      "\t|dl_db = [[0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "dnn = DNN()\n",
    "\n",
    "dnn.set_nodes(x=2,k=3,h=3,o=2,y=2)\n",
    "dnn.set_weights_W()\n",
    "dnn.set_weights_V()\n",
    "# dnn.default_init()*\n",
    "dnn.train_epoch(rounds=0,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_do: [[-0.5], [0.5]] \t shape: (2, 1)\n",
      "dl_do: [[-0.5], [0.5]] \t shape: (2, 1)\n",
      "h    : [[0.8807970779778823], [0.8807970779778823], [0.8807970779778823]] \t shape: (3, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,1) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4036/1543765145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"h    : {dnn.h.tolist()} \\t shape: {dnn.h.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\ndl_do*h = {(dl_do * dnn.h).tolist()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# print(f\"\\ndl_do*h = {(dnn.dl_do * dnn.h).tolist()}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,1) (3,1) "
     ]
    }
   ],
   "source": [
    "print(f\"dl_do: {dnn.dl_do.tolist()} \\t shape: {dnn.dl_do.shape}\")\n",
    "dl_do = dnn.dl_do.reshape(2,1)\n",
    "print(f\"dl_do: {dl_do.tolist()} \\t shape: {dl_do.shape}\")\n",
    "\n",
    "print(f\"h    : {dnn.h.tolist()} \\t shape: {dnn.h.shape}\")\n",
    "print(f\"\\ndl_do*h = {(dl_do * dnn.h).tolist()}\")\n",
    "# print(f\"\\ndl_do*h = {(dnn.dl_do * dnn.h).tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79639203, 0.79639203],\n",
       "       [0.79639203, 0.79639203],\n",
       "       [0.79639203, 0.79639203]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dnn.h.shape\n",
    "np.tile(dnn.h,(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[2], [4]]\n",
      "y = [[2, 2], [4, 4]]\n",
      "z = [[4, 4], [16, 16]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2],[4]])  #(2,1) \n",
    "y = np.array([[2,2],[4,4]])  #(2,2) \n",
    "\n",
    "print(f\"x = {x.tolist()}\")\n",
    "print(f\"y = {y.tolist()}\")\n",
    "\n",
    "z = x*y\n",
    "\n",
    "print(f\"z = {z.tolist()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances:     60000\n",
      "number of input nodes:            2\n",
      "(given) number of output nodes:   10\n",
      "(given) hidden layer size:        300\n"
     ]
    }
   ],
   "source": [
    "# type(q5_dnn.xtrain)\n",
    "# print(np.unique(q5_dnn.xtrain))\n",
    "print(f\"number of training instances:     {len(dnn.xtrain)}\")\n",
    "print(f\"number of input nodes:            {len(dnn.xtrain[0])}\")\n",
    "print(f\"(given) number of output nodes:   10\")\n",
    "print(f\"(given) hidden layer size:        300\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1d88f1d6af7274392319340ad589157e5034eb25853bd7ff5b502ff0dd39369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
