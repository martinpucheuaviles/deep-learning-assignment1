{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import exp, fsum, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scalar Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + exp(-x))\n",
    "\n",
    "def softmax(o,i):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -o : vector of softmax input\n",
    "      -i : index i of o\n",
    "  \"\"\"\n",
    "  return exp(o[i]) / fsum([exp(oj) for oj in o])\n",
    "\n",
    "def cross_entropy_loss(y,t):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -y: vector y output of softmax\n",
    "      -t: target vector\n",
    "  \"\"\"\n",
    "  return -1 * log(y[t.index(1)])\n",
    "\n",
    "def scalar_dl_dy(y,t,i):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -y: vector output of softmax\n",
    "      -t: target vector\n",
    "      -i: index of y\n",
    "  \"\"\"\n",
    "  if i == t.index(1):\n",
    "      dl_dy_i = -1.0 / y[i]\n",
    "  else:\n",
    "      dl_dy_i = 0.0  \n",
    "  \n",
    "  return dl_dy_i\n",
    "\n",
    "def scalar_dy_do(y,i,j):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -y: vector output of softmax\n",
    "      -i: index of y\n",
    "      -j: index of o ( o = h*V +c)\n",
    "  \"\"\"\n",
    "  if i==j:\n",
    "    dy_do = y[i]*(1-y[i])\n",
    "  else:\n",
    "    dy_do = -1*y[i]*y[j]\n",
    "  \n",
    "  return dy_do\n",
    "\n",
    "def scalar_dl_do(dl_dy,dy_do,i):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -dl_dy: vector of derivates of the loss wrt softmax output\n",
    "      -dy_do: matrix with derivatives of softmax output wrt softmax input ( o = h*V +c)\n",
    "      -i    : index of o\n",
    "  \"\"\"\n",
    "  dl_do_i = 0.0\n",
    "  for k,dl_dy[k] in enumerate(dl_dy):\n",
    "    \n",
    "    dl_do_i += dl_dy[k]*dy_do[k][i]\n",
    "\n",
    "  return dl_do_i\n",
    "\n",
    "def scalar_do_dh(V,i,j):\n",
    "  \"\"\"\n",
    "      Parameters:\n",
    "        -V: matrix of weights from sigmoid output to softmax input\n",
    "        -i: index of o (softmax input)\n",
    "        -j: index of h (sigmoid output)\n",
    "    \"\"\"\n",
    "  return V[j][i]\n",
    "\n",
    "def scalar_dl_dh(dl_do,do_dh,i):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -dl_do: vector of derivates of the loss wrt softmax input\n",
    "      -do_dh: matrix with derivatives of softmax input wrt sigmoid output\n",
    "      -i    : index of k (sigmoid output)\n",
    "  \"\"\"\n",
    "  dl_dh_i = 0.0\n",
    "  for j,dl_do[j] in enumerate(dl_do):\n",
    "    \n",
    "    dl_dh_i += dl_do[j]*do_dh[j][i]\n",
    "\n",
    "  return dl_dh_i  \n",
    "\n",
    "\n",
    "\n",
    "def scalar_dl_dv(dl_do,do_dv_ij,j):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "      -dl_do    : vector of derivatives of the loss wrt softmax inputs\n",
    "      -do_dv_ij : scalar derivative of the ith-element of softmac inputs wrt the ij_th elements of weights V\n",
    "      -j        : column index of weights V\n",
    "  \"\"\"\n",
    "  \n",
    "  return dl_do[j]*do_dv_ij\n",
    "\n",
    "def scalar_dl_dc(dl_do,do_dc_i,i):\n",
    "  \"\"\"\n",
    "    -Parameters:\n",
    "      -dl_do    : derivative of the loss wrt softmax input\n",
    "      -do_dc_i  : derivative of softmax inpuot with respect to weight c (always 1)\n",
    "      -i        : index of softmax input\n",
    "  \"\"\"\n",
    "  return dl_do[i]*do_dc_i\n",
    "\n",
    "def scalar_dh_dk(k,i):\n",
    "  \"\"\"\n",
    "    -Parameters:\n",
    "      -k: vector linear input of sigmoid\n",
    "      -i: index of k\n",
    "  \"\"\"\n",
    "  return sigmoid(k[i])*(1-sigmoid(k[i]))\n",
    "\n",
    "def scalar_dl_dk(dl_dh,dh_dk,i):\n",
    "  \"\"\"\n",
    "    -Parameters:\n",
    "      -dl_dh: derivative of the loss wrt sigmoid output\n",
    "      -dh_dk: derivative of the sigmoid output wrt to sigmoid input\n",
    "      -i    : index of sigmoid input k\n",
    "  \"\"\"\n",
    "  return dl_dh[i]*dh_dk[i]\n",
    "\n",
    "def scalar_dl_dw(dl_dk, dk_dw_ij,j):\n",
    "  \"\"\"\n",
    "      -dl_dk    : vector of derivatives of the loss wrt sigmoid inputs\n",
    "      -dk_dw_ij : scalar derivative of the ith-element of sigmoid inputs wrt the ij_th elements of weights W\n",
    "      -j        : column index of weights W\n",
    "  \"\"\"\n",
    "\n",
    "  return dl_dk[j]*dk_dw_ij\n",
    "\n",
    "def scalar_dl_db(dl_dk,dk_db_i,i):\n",
    "  \"\"\"\n",
    "    -Parameters:\n",
    "      -dl_dk    : derivative of the loss wrt sigmoid input\n",
    "      -dk_db_i  : derivative of sigmoid input with respect to weight b (always 1)\n",
    "      -i        : index of sigmoid input\n",
    "  \"\"\"\n",
    "  return dl_dk[i]*dk_db_i  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \"\"\" Initialize lists of inputs, bias, weights and target class\"\"\"\n",
    "    x = [1.0,-1.0]              #inputs\n",
    "\n",
    "    W = [[1.0, 1.0, 1.0],\n",
    "        [-1.0, -1.0, -1.0]\n",
    "        ]                       #input weights\n",
    "\n",
    "    b = [0.0, 0.0, 0.0]         # input-bias\n",
    "\n",
    "    k = [0.0, 0.0, 0.0]         # linear outputs  (x * W + b )\n",
    "\n",
    "    h = [0.0, 0.0, 0.0]         # sigmoid activation outputs\n",
    "\n",
    "    c = [0.0, 0.0]              # constant-bias \n",
    "\n",
    "    V = [[1.0, 1.0],\n",
    "        [-1.0, -1.0],\n",
    "        [-1.0, -1.0]\n",
    "        ]                       # sigmoid-output weights\n",
    "\n",
    "    o = [0.0, 0.0]              # h * V + c\n",
    "\n",
    "    y = [0.0, 0.0]              # softmax activated output\n",
    "\n",
    "    t = [1,0]                   #target class\n",
    "    \n",
    "    return x,W,b,k,h,c,V,o,y,t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Linear non-activated outputs: \n",
      "k = [2.0, 2.0, 2.0]\n",
      "\n",
      "-Sigmoid activated outputs: \n",
      "h = [0.8807970779778823, 0.8807970779778823, 0.8807970779778823]\n",
      "\n",
      "-Soft-Max activation : \n",
      "y=[0.5, 0.5]\n",
      "\n",
      "Loss = -log(y_c) = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "x,W,b,k,h,c,V,o,y,t = initialize()\n",
    "\n",
    "# K = X * W + b\n",
    "for j in range(0,len(k)):     # Len of K-nodes\n",
    "    for i in range(0,len(x)): # number of inputs\n",
    "        k[j] += W[i][j] * x[i]\n",
    "\n",
    "    k[j] += b[j]\n",
    "\n",
    "    h[j] = sigmoid(k[j])\n",
    "\n",
    "#SoftMax Activation\n",
    "for j in range(0,len(y)):\n",
    "    for i in range(0,len(h)):\n",
    "        o[j] += h[i] * V[i][j]\n",
    "\n",
    "    o[j] += c[j]\n",
    "\n",
    "for j in range(0,len(y)):\n",
    "    y[j] = softmax(o,j)\n",
    "\n",
    "\n",
    "l = cross_entropy_loss(y,t)\n",
    "\n",
    "\n",
    "print(f\"-Linear non-activated outputs: \\nk = {k}\\n\")\n",
    "print(f\"-Sigmoid activated outputs: \\nh = {h}\\n\")\n",
    "print(f\"-Soft-Max activation : \\ny={y}\\n\")\n",
    "print(f\"Loss = -log(y_c) = {l}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_dy = [-2.0, 0.0]\n",
      "dy_do = [[0.25, -0.25], [-0.25, 0.25]]\n",
      "dl_do = [-0.5, 0.5]\n",
      "do_dh = [[1.0, -1.0, -1.0], [1.0, -1.0, -1.0]]\n",
      "dl_dh = [0.0, 0.0, 0.0]\n",
      "dl_dv = [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "dl_dc = [-0.5, 0.5]\n",
      "dh_dk = [0.10499358540350662, 0.10499358540350662, 0.10499358540350662]\n",
      "dl_dk = [0.0, 0.0, 0.0]\n",
      "dl_dw = [[0.0, 0.0, 0.0], [-0.0, -0.0, -0.0]]\n",
      "dl_db = [0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ############# DL / DY #############\"\"\"\n",
    "dl_dy = [0.0, 0.0] #derivatives of the loss w.r.t softmax output\n",
    "\n",
    "for i in range(0,len(dl_dy)):\n",
    "\n",
    "    dl_dy[i] = scalar_dl_dy(y,t,i)\n",
    "\n",
    "print(f\"dl_dy = {dl_dy}\")\n",
    "\n",
    "\n",
    "\"\"\" ############# DY / DO #############\"\"\"\n",
    "\n",
    "dy_do = [[0.0, 0.0],[0.0, 0.0]] #derivatives of the loss w.r.t softmax output\n",
    "\n",
    "for i,y_i in enumerate(y):\n",
    "    for j,o_j in enumerate(o):\n",
    "        dy_do[i][j] = scalar_dy_do(y,i,j)\n",
    "\n",
    "print(f\"dy_do = {dy_do}\")\n",
    "\n",
    "\"\"\" ############# DL / DO ############# \"\"\"\n",
    "\n",
    "dl_do = [0.0, 0.0]\n",
    "\n",
    "for i in range(0,len(dl_do)):\n",
    "\n",
    "    dl_do[i] = scalar_dl_do(dl_dy,dy_do,i)\n",
    "\n",
    "print(f\"dl_do = {dl_do}\")\n",
    "\n",
    "\"\"\" ############# DO / DH #############\"\"\"\n",
    "\n",
    "do_dh = [[0.0,0.0,0.0],[0.0,0.0,0.0]]\n",
    "\n",
    "for i,o_i in enumerate(o):\n",
    "    for j,k_j in enumerate(k):\n",
    "        do_dh[i][j] = scalar_do_dh(V,i,j)\n",
    "\n",
    "print(f\"do_dh = {do_dh}\")\n",
    "\n",
    "\"\"\" ############# DL / DH #############\"\"\"\n",
    "\n",
    "dl_dh = [0.0, 0.0, 0.0]\n",
    "\n",
    "\n",
    "for i,k[i] in enumerate(k):\n",
    "    dl_dh[i] = scalar_dl_dh(dl_do,do_dh,i)\n",
    "\n",
    "print(f\"dl_dh = {dl_dh}\")\n",
    "\n",
    "\"\"\" ############# DL / DV #############\"\"\"\n",
    "\n",
    "dl_dv = [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]\n",
    "\n",
    "for i,h_i in enumerate(h): # V edges come out from h-nodes\n",
    "    for j, __ in enumerate(dl_dv[i]):\n",
    "        dl_dv[i][j] = scalar_dl_dv(dl_do, h_i, j)  # dOi_dVij is just h[i]\n",
    "\n",
    "\n",
    "print(f\"dl_dv = {dl_dv}\")\n",
    "\n",
    "\"\"\" ############# DL / DC #############\"\"\"\n",
    "dl_dc = [0.0, 0.0]\n",
    "\n",
    "for i,c_i in enumerate(c):\n",
    "    do_dc_i  = 1  # O_i = sum_j(h_j*V_ji) + c_i  ==> do_i/dc_i = 1 for any i\n",
    "    dl_dc[i] = scalar_dl_dc(dl_do,do_dc_i,i)\n",
    "\n",
    "print(f\"dl_dc = {dl_dc}\")\n",
    "\n",
    "\n",
    "\"\"\" ############# DL / DK #############\"\"\"\n",
    "\"\"\"   -> using DH/DK\"\"\"    \n",
    "\n",
    "dl_dk = [0.0, 0.0, 0.0]\n",
    "dh_dk = [0.0, 0.0, 0.0]  #only interested in same i-index e.g dHi/dKi\n",
    "for i, k_i in enumerate(k):\n",
    "    dh_dk[i] = scalar_dh_dk(k,i)\n",
    "\n",
    "for i,k_i in enumerate(k):\n",
    "    dl_dk[i] = scalar_dl_dk(dl_dh,dh_dk,i)\n",
    "\n",
    "print(f\"dh_dk = {dh_dk}\")\n",
    "print(f\"dl_dk = {dl_dk}\")\n",
    "\n",
    "\"\"\" ############# DL / DW #############\"\"\"\n",
    "\n",
    "dl_dw = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n",
    "\n",
    "for i, x_i in enumerate(x): # W edges come out from x-nodes\n",
    "    for j, __ in enumerate(dl_dw[i]):\n",
    "        dl_dw[i][j] = scalar_dl_dw(dl_dk, x_i,j) # dKi_dWij is just x[i]\n",
    "\n",
    "print(f\"dl_dw = {dl_dw}\")\n",
    "\n",
    "\"\"\" ############# DL / DB ############# \"\"\"\n",
    "\n",
    "dl_db = [0.0, 0.0, 0.0]\n",
    "\n",
    "for i,b_i in enumerate(b):\n",
    "    dk_db_i  = 1  # K_i = sum_j(x_j*W_ji) + b_i  ==> dk_i/db_i = 1 for any i\n",
    "    dl_db[i] = scalar_dl_db(dl_dk,dk_db_i,i)\n",
    "\n",
    "print(f\"dl_db = {dl_db}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_synth\n",
    "import matplotlib.pyplot as plt \n",
    "(xtrain, ytrain), (xval, yval), num_cls = load_synth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q4():\n",
    "    random.seed(42)\n",
    "    # x = [1.0,-1.0]              #inputs\n",
    "\n",
    "    W = [[random.random() for w in range(0,3)] for _ in range(0,2)]\n",
    "\n",
    "    b = [0.0, 0.0, 0.0]         # input-bias\n",
    "\n",
    "    k = [0.0, 0.0, 0.0]         # linear outputs  (x * W + b )\n",
    "\n",
    "    h = [0.0, 0.0, 0.0]         # sigmoid activation outputs\n",
    "\n",
    "    c = [0.0, 0.0]              # constant-bias \n",
    "\n",
    "    V = [[random.random() for w in range(0,2)] for _ in range(0,3)] # sigmoid-output weights\n",
    "\n",
    "    o = [0.0, 0.0]              # h * V + c\n",
    "\n",
    "    y = [0.0, 0.0]              # softmax activated output\n",
    "\n",
    "    # t = [1,0]                   #target class\n",
    "    \n",
    "    return W,b,k,h,c,V,o,y    \n",
    "\n",
    "def train(x,t,W,b,k,h,c,V,o,y,alpha=0.02):\n",
    "    \"\"\" \n",
    "        Parameters:\n",
    "            - alpha: learning rate\n",
    "    \"\"\"\n",
    "    \"\"\" ######################################## FORWARD #####################################\"\"\"\n",
    "\n",
    "    # K = X * W + b\n",
    "    for j in range(0,len(k)):     # Len of K-nodes\n",
    "        for i in range(0,len(x)): # number of inputs\n",
    "            k[j] += W[i][j] * x[i]\n",
    "\n",
    "        k[j] += b[j]\n",
    "\n",
    "        h[j] = sigmoid(k[j])\n",
    "\n",
    "    #SoftMax Activation\n",
    "    for j in range(0,len(y)):\n",
    "        for i in range(0,len(h)):\n",
    "            o[j] += h[i] * V[i][j]\n",
    "\n",
    "        o[j] += c[j]\n",
    "\n",
    "    for j in range(0,len(y)):\n",
    "        y[j] = softmax(o,j)\n",
    "\n",
    "\n",
    "    l = cross_entropy_loss(y,t)\n",
    "\n",
    "    \"\"\" ######################################## BACKWARD #####################################\"\"\"\n",
    "\n",
    "    dl_dy = [0.0, 0.0] #derivatives of the loss w.r.t softmax output\n",
    "    dy_do = [[0.0, 0.0],[0.0, 0.0]] #derivatives of the loss w.r.t softmax output\n",
    "    dl_do = [0.0, 0.0]\n",
    "    do_dh = [[0.0,0.0,0.0],[0.0,0.0,0.0]]\n",
    "    dl_dh = [0.0, 0.0, 0.0]\n",
    "    dl_dv = [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]\n",
    "    dl_dc = [0.0, 0.0]\n",
    "    dl_dk = [0.0, 0.0, 0.0]\n",
    "    dh_dk = [0.0, 0.0, 0.0]  #only interested in same i-index e.g dHi/dKi\n",
    "    dl_dw = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n",
    "    dl_db = [0.0, 0.0, 0.0]\n",
    "\n",
    "    for i in range(0,len(dl_dy)):\n",
    "        dl_dy[i] = scalar_dl_dy(y,t,i)\n",
    "\n",
    "    for i,_ in enumerate(y):\n",
    "        for j,__ in enumerate(o):\n",
    "            dy_do[i][j] = scalar_dy_do(y,i,j)\n",
    "\n",
    "    for i in range(0,len(dl_do)):\n",
    "        dl_do[i] = scalar_dl_do(dl_dy,dy_do,i)\n",
    "\n",
    "    for i,_ in enumerate(o):\n",
    "        for j,__ in enumerate(k):\n",
    "            do_dh[i][j] = scalar_do_dh(V,i,j)\n",
    "\n",
    "    for i,k[i] in enumerate(k):\n",
    "        dl_dh[i] = scalar_dl_dh(dl_do,do_dh,i)\n",
    "\n",
    "    for i,h_i in enumerate(h): # V edges come out from h-nodes\n",
    "        for j, _ in enumerate(dl_dv[i]):\n",
    "            dl_dv[i][j] = scalar_dl_dv(dl_do, h_i, j)  # dOi_dVij is just h[i]\n",
    "\n",
    "    for i,_ in enumerate(c):\n",
    "        do_dc_i  = 1  # O_i = sum_j(h_j*V_ji) + c_i  ==> do_i/dc_i = 1 for any i\n",
    "        dl_dc[i] = scalar_dl_dc(dl_do,do_dc_i,i)\n",
    "\n",
    "    for i, _ in enumerate(k):\n",
    "        dh_dk[i] = scalar_dh_dk(k,i)\n",
    "\n",
    "    for i,_ in enumerate(k):\n",
    "        dl_dk[i] = scalar_dl_dk(dl_dh,dh_dk,i)\n",
    "\n",
    "    for i, x_i in enumerate(x): # W edges come out from x-nodes\n",
    "        for j, __ in enumerate(dl_dw[i]):\n",
    "            dl_dw[i][j] = scalar_dl_dw(dl_dk, x_i,j) # dKi_dWij is just x[i]\n",
    "\n",
    "    for i,_ in enumerate(b):\n",
    "        dk_db_i  = 1  # K_i = sum_j(x_j*W_ji) + b_i  ==> dk_i/db_i = 1 for any i\n",
    "        dl_db[i] = scalar_dl_db(dl_dk,dk_db_i,i)\n",
    "\n",
    "    \n",
    "    \"\"\" ######################################## UPDATING #####################################\"\"\"\n",
    "\n",
    "    for i, w_i in enumerate(W):\n",
    "        for j, w_ij in enumerate(w_i):\n",
    "            W[i][j] = W[i][j] - alpha * dl_dw[i][j]\n",
    "\n",
    "    for i, b_i in enumerate(b):\n",
    "        b[i] = b[i] - alpha * dl_db[i]\n",
    "\n",
    "    for i, v_i in enumerate(V):\n",
    "        for j, v_ij in enumerate(v_i):\n",
    "            V[i][j] = V[i][j] - alpha * dl_dv[i][j]\n",
    "\n",
    "    for i, c_i in enumerate(c):\n",
    "        c[i] = c[i] - alpha * dl_dc[i]\n",
    "\n",
    "    return W,b,k,h,c,V,o,y,l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-8e839bed19cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mt_train_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_train_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# print(f\"xtrain = {xtrain_i}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# print(f\"target = {t_train_i}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-358129fb2ef6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x, t, W, b, k, h, c, V, o, y, alpha)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m#SoftMax Activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-5879a408272c>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[1;32massert\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"ERROR x = {x}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "W,b,k,h,c,V,o,y = initialize_q4() #initialize NN\n",
    "losses = []\n",
    "counter = 0\n",
    "for i,xtrain_i in enumerate(xtrain):\n",
    "    t_train_i = [1,0] if ytrain[i] == 1 else [0,1]\n",
    "    \n",
    "    W,b,k,h,c,V,o,y,loss = train(xtrain_i,t_train_i,W,b,k,h,c,V,o,y,alpha=0.05)\n",
    "    # print(f\"xtrain = {xtrain_i}\")\n",
    "    # print(f\"target = {t_train_i}\")\n",
    "    # print(f\"prediction y = {y}\")\n",
    "    # print(f\"->LOSS = {loss}\\n\")\n",
    "\n",
    "    losses.append(loss)\n",
    "    # if counter < 10000:\n",
    "    #     counter += 1\n",
    "    # else:\n",
    "    #     break\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6394267984578837, 0.025010755222666936, 0.27502931836911926],\n",
       " [0.22321073814882275, 0.7364712141640124, 0.6766994874229113]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.6394267984578837, 0.025010755222666936, 0.27502931836911926]\n",
      "\n",
      "1\n",
      "[0.22321073814882275, 0.7364712141640124, 0.6766994874229113]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    print(i)\n",
    "    print(w)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef40b6422afe8683db2cb0092dfd2afc9fc1d4ff8f547db7f19c45554c3c28be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
